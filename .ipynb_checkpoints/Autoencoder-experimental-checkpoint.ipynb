{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "random_wav = \"bass_electronic_018-022-100.wav\"\n",
    "\n",
    "\n",
    "audio = utils.load_audio(random_wav)\n",
    "sample_length = audio.shape[0]\n",
    "print(sample_length)\n",
    "audio = np.concatenate((audio, audio))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJamGDH9JqqH/D6RlUGqnTbcaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12722f01d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(audio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   6.10351562e-05   2.13623047e-04 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mul_or_none(a, b):\n",
    "  \"\"\"Return the element wise multiplicative of the inputs.\n",
    "\n",
    "  If either input is None, we return None.\n",
    "\n",
    "  Args:\n",
    "    a: A tensor input.\n",
    "    b: Another tensor input with the same type as a.\n",
    "\n",
    "  Returns:\n",
    "    None if either input is None. Otherwise returns a * b.\n",
    "  \"\"\"\n",
    "  if a is None or b is None:\n",
    "    return None\n",
    "  return a * b\n",
    "\n",
    "def time_to_batch(x, block_size):\n",
    "  \"\"\"Splits time dimension (i.e. dimension 1) of `x` into batches.\n",
    "\n",
    "  Within each batch element, the `k*block_size` time steps are transposed,\n",
    "  so that the `k` time steps in each output batch element are offset by\n",
    "  `block_size` from each other.\n",
    "\n",
    "  The number of input time steps must be a multiple of `block_size`.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor of shape [nb, k*block_size, n] for some natural number k.\n",
    "    block_size: number of time steps (i.e. size of dimension 1) in the output\n",
    "      tensor.\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape [nb*block_size, k, n]\n",
    "  \"\"\"\n",
    "  shape = x.get_shape().as_list()\n",
    "  y = tf.reshape(x, [\n",
    "      shape[0], shape[1] // block_size, block_size, shape[2]\n",
    "  ])\n",
    "  y = tf.transpose(y, [0, 2, 1, 3])\n",
    "  y = tf.reshape(y, [\n",
    "      shape[0] * block_size, shape[1] // block_size, shape[2]\n",
    "  ])\n",
    "  y.set_shape([\n",
    "      mul_or_none(shape[0], block_size), mul_or_none(shape[1], 1. / block_size),\n",
    "      shape[2]\n",
    "  ])\n",
    "  return y\n",
    "\n",
    "\n",
    "def batch_to_time(x, block_size):\n",
    "  \"\"\"Inverse of `time_to_batch(x, block_size)`.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor of shape [nb*block_size, k, n] for some natural number k.\n",
    "    block_size: number of time steps (i.e. size of dimension 1) in the output\n",
    "      tensor.\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape [nb, k*block_size, n].\n",
    "  \"\"\"\n",
    "  shape = x.get_shape().as_list()\n",
    "  y = tf.reshape(x, [shape[0] // block_size, block_size, shape[1], shape[2]])\n",
    "  y = tf.transpose(y, [0, 2, 1, 3])\n",
    "  y = tf.reshape(y, [shape[0] // block_size, shape[1] * block_size, shape[2]])\n",
    "  y.set_shape([mul_or_none(shape[0], 1. / block_size),\n",
    "               mul_or_none(shape[1], block_size),\n",
    "               shape[2]])\n",
    "  return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-afedf79e884b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#encode with 8 bit mu-law\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mx_quantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_law\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_quantized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m128.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "learning_rate = 0.00001\n",
    "num_lstm = 512\n",
    "with tf.Graph().as_default():\n",
    "    #session_config = tf.Config\n",
    "    with tf.Session() as sess:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_lstm)\n",
    "        #initial_state = tf.zeros([batch_size, lstm.state_size])\n",
    "        \n",
    "#encode with 8 bit mu-law\n",
    "x = audio\n",
    "x_quantized = utils.mu_law(x)\n",
    "x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n",
    "#x_scaled = tf.expand_dims(x_scaled, 2)\n",
    "num_z = 16\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "num_steps = 1\n",
    "input_size = 1\n",
    "lstm_size = 128\n",
    "keep_prob = 1\n",
    "stock_count = 1\n",
    "embed_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#official tutorial: words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\n",
    "learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "# symbols are mapped to integers.\n",
    "symbols = tf.placeholder(tf.int32, [None, 1], name='symbols')\n",
    "#inputs = tf.placeholder(tf.float32, [None, num_steps, input_size], name=\"inputs\")\n",
    "inputs = tf.placeholder(tf.float32, [None], name=\"inputs\")\n",
    "inputs = tf.reshape(inputs, tf.TensorShape([64000,1,1]))\n",
    "targets = tf.placeholder(tf.float32, [None, input_size], name=\"targets\")\n",
    "\n",
    "def _create_one_cell():\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "    '''\n",
    "    if keep_prob < 1.0:\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)    '''\n",
    "    return lstm_cell\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([_create_one_cell() for _ in range(num_layers)],state_is_tuple=True\n",
    ") if num_layers > 1 else _create_one_cell()\n",
    "\n",
    "\n",
    "#embedding layer\n",
    "embed_matrix = tf.Variable(\n",
    "    tf.random_uniform([stock_count, embed_size], -1.0, 1.0),\n",
    "    name=\"embed_matrix\"\n",
    ")\n",
    "sym_embeds = tf.nn.embedding_lookup(embed_matrix, symbols)\n",
    "            \n",
    "# stock_label_embeds.shape = (batch_size, embedding_size)\n",
    "stacked_symbols = tf.tile(symbols, [1, num_steps], name='stacked_stock_labels')\n",
    "stacked_embeds = tf.nn.embedding_lookup(embed_matrix, stacked_symbols)\n",
    "\n",
    "# After concat, inputs.shape = (batch_size, num_steps, lstm_size + embed_size)\n",
    "inputs_with_embed = tf.concat([inputs, stacked_embeds], axis=2, name=\"inputs_with_embed\")\n",
    "\n",
    "val, state_ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, scope=\"dynamic_rnn\")\n",
    "\n",
    "# Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size)\n",
    "# After transpose, val.get_shape() = (num_steps, batch_size, lstm_size)\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1, name=\"lstm_state\")\n",
    "ws = tf.Variable(tf.truncated_normal([lstm_size, input_size]), name=\"w\")\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[input_size]), name=\"b\")\n",
    "pred = tf.matmul(last, ws) + bias\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(pred - targets), name=\"loss_mse\")\n",
    "optim = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, name=\"rmsprop_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object dt.generate_one_epoch at 0x7f455407e0a0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio = utils.load_audio(random_wav)\n",
    "audio = list(map(lambda x: x, audio))\n",
    "np.array(audio)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class dt():\n",
    "    def __init__(self, raw_audio, input_size = input_size, num_steps = num_steps, test_ratio = 0.1, normalized = True):\n",
    "        self.raw_seq = np.array(raw_audio)\n",
    "        self.input_size = input_size\n",
    "        self.num_steps = num_steps \n",
    "        self.test_ratio = test_ratio\n",
    "        self.normalized = normalized\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self._prepare_data(self.raw_seq)\n",
    "\n",
    "    def _prepare_data(self, seq):\n",
    "        # split into items of input_size\n",
    "        seq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size])\n",
    "               for i in range(len(seq) // self.input_size)]\n",
    "\n",
    "        if self.normalized:\n",
    "            seq = [seq[0] / seq[0][0] - 1.0] + [\n",
    "                curr / seq[i][-1] - 1.0 for i, curr in enumerate(seq[1:])]\n",
    "\n",
    "        # split into groups of num_steps\n",
    "        X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "        y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.test_ratio))\n",
    "        train_X, test_X = X[:train_size], X[train_size:]\n",
    "        train_y, test_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, test_X, test_y\n",
    "\n",
    "    def generate_one_epoch(self, batch_size):\n",
    "        num_batches = int(len(self.train_X)) // batch_size\n",
    "        if batch_size * num_batches < len(self.train_X):\n",
    "            num_batches += 1\n",
    "\n",
    "        batch_indices = range(num_batches)\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = self.train_X[j * batch_size: (j + 1) * batch_size]\n",
    "            batch_y = self.train_y[j * batch_size: (j + 1) * batch_size]\n",
    "            assert set(map(len, batch_X)) == {self.num_steps}\n",
    "            yield batch_X, batch_y\n",
    "\n",
    "d = dt(audio)\n",
    "d.generate_one_epoch(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64000,) for Tensor 'Reshape:0', which has shape '(64000, 1, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-2dc810bde5dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0msymbols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmerged_test_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         }\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64000,) for Tensor 'Reshape:0', which has shape '(64000, 1, 1)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    merged_test_labels = [1] * len(audio)\n",
    "    merged_test_X = np.array(audio)\n",
    "    merged_test_y = np.array(audio)\n",
    "    merged_test_labels = np.array(merged_test_labels)\n",
    "    train_data_feed = {\n",
    "            inputs: merged_test_X,\n",
    "            targets: merged_test_y,\n",
    "            symbols: merged_test_labels,\n",
    "        }\n",
    "    train_loss, _ = sess.run([loss, optim], train_data_feed)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv_3:0\", shape=(128000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class LstmRNN(object):\n",
    "    def __init__(self, sess, stock_count,\n",
    "                 lstm_size=128,\n",
    "                 num_layers=1,\n",
    "                 num_steps=30,\n",
    "                 input_size=1,\n",
    "                 keep_prob=0.8,\n",
    "                 embed_size=None,\n",
    "                 logs_dir=\"logs\",\n",
    "                 plots_dir=\"images\"):\n",
    "        \"\"\"\n",
    "        Construct a RNN model using LSTM cell.\n",
    "\n",
    "        Args:\n",
    "            sess:\n",
    "            stock_count:\n",
    "            lstm_size:\n",
    "            num_layers\n",
    "            num_steps:\n",
    "            input_size:\n",
    "            keep_prob:\n",
    "            embed_size\n",
    "            checkpoint_dir\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.stock_count = stock_count\n",
    "\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "        self.input_size = input_size\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        self.use_embed = (embed_size is not None) and (embed_size > 0)\n",
    "        self.embed_size = embed_size or -1\n",
    "\n",
    "        self.logs_dir = logs_dir\n",
    "        self.plots_dir = plots_dir\n",
    "\n",
    "        self.build_graph()\n",
    "\n",
    "    def train(self, dataset_list, config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_list (<StockDataSet>)\n",
    "            config (tf.app.flags.FLAGS)\n",
    "        \"\"\"\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # Merged test data of different stocks.\n",
    "        merged_test_X = []\n",
    "        merged_test_y = []\n",
    "        merged_test_labels = []\n",
    "\n",
    "        for label_, d_ in enumerate(dataset_list):\n",
    "            merged_test_X += list(d_.test_X)\n",
    "            merged_test_y += list(d_.test_y)\n",
    "            merged_test_labels += [[label_]] * len(d_.test_X)\n",
    "\n",
    "        merged_test_X = np.array(merged_test_X)\n",
    "        merged_test_y = np.array(merged_test_y)\n",
    "        merged_test_labels = np.array(merged_test_labels)\n",
    "\n",
    "        print \"len(merged_test_X) =\", len(merged_test_X)\n",
    "        print \"len(merged_test_y) =\", len(merged_test_y)\n",
    "        print \"len(merged_test_labels) =\", len(merged_test_labels)\n",
    "\n",
    "        test_data_feed = {\n",
    "            self.learning_rate: 0.0,\n",
    "            self.inputs: merged_test_X,\n",
    "            self.targets: merged_test_y,\n",
    "            self.symbols: merged_test_labels,\n",
    "        }\n",
    "\n",
    "        global_step = 0\n",
    "\n",
    "        num_batches = sum(len(d_.train_X) for d_ in dataset_list) // config.batch_size\n",
    "        random.seed(time.time())\n",
    "\n",
    "        # Select samples for plotting.\n",
    "        sample_labels = range(min(config.sample_size, len(dataset_list)))\n",
    "        sample_indices = {}\n",
    "        for l in sample_labels:\n",
    "            sym = dataset_list[l].stock_sym\n",
    "            target_indices = np.array([\n",
    "                i for i, sym_label in enumerate(merged_test_labels)\n",
    "                if sym_label[0] == l])\n",
    "            sample_indices[sym] = target_indices\n",
    "        print sample_indices\n",
    "\n",
    "        print \"Start training for stocks:\", [d.stock_sym for d in dataset_list]\n",
    "        for epoch in xrange(config.max_epoch):\n",
    "            epoch_step = 0\n",
    "            learning_rate = config.init_learning_rate * (\n",
    "                config.learning_rate_decay ** max(float(epoch + 1 - config.init_epoch), 0.0)\n",
    "            )\n",
    "\n",
    "            for label_, d_ in enumerate(dataset_list):\n",
    "                for batch_X, batch_y in d_.generate_one_epoch(config.batch_size):\n",
    "                    global_step += 1\n",
    "                    epoch_step += 1\n",
    "                    batch_labels = np.array([[label_]] * len(batch_X))\n",
    "                    train_data_feed = {\n",
    "                        self.learning_rate: learning_rate,\n",
    "                        self.inputs: batch_X,\n",
    "                        self.targets: batch_y,\n",
    "                        self.symbols: batch_labels,\n",
    "                    }\n",
    "                    train_loss, _, train_merged_sum = self.sess.run(\n",
    "                        [self.loss, self.optim, self.merged_sum], train_data_feed)\n",
    "                    self.writer.add_summary(train_merged_sum, global_step=global_step)\n",
    "\n",
    "                    if np.mod(global_step, len(dataset_list) * 100 / config.input_size) == 1:\n",
    "                        test_loss, test_pred = self.sess.run([self.loss, self.pred], test_data_feed)\n",
    "\n",
    "                        print \"Step:%d [Epoch:%d] [Learning rate: %.6f] train_loss:%.6f test_loss:%.6f\" % (\n",
    "                            global_step, epoch, learning_rate, train_loss, test_loss)\n",
    "\n",
    "                        # Plot samples\n",
    "                        for sample_sym, indices in sample_indices.iteritems():\n",
    "                            image_path = os.path.join(self.model_plots_dir, \"{}_epoch{:02d}_step{:04d}.png\".format(\n",
    "                                sample_sym, epoch, epoch_step))\n",
    "                            sample_preds = test_pred[indices]\n",
    "                            sample_truth = merged_test_y[indices]\n",
    "                            self.plot_samples(sample_preds, sample_truth, image_path, stock_sym=sample_sym)\n",
    "\n",
    "                        self.save(global_step)\n",
    "\n",
    "        final_pred, final_loss = self.sess.run([self.pred, self.loss], test_data_feed)\n",
    "\n",
    "        # Save the final model\n",
    "        self.save(global_step)\n",
    "        return final_pred\n",
    "\n",
    "    @property\n",
    "    def model_name(self):\n",
    "        name = \"stock_rnn_lstm%d_step%d_input%d\" % (\n",
    "            self.lstm_size, self.num_steps, self.input_size)\n",
    "\n",
    "        if self.embed_size > 0:\n",
    "            name += \"_embed%d\" % self.embed_size\n",
    "\n",
    "        return name\n",
    "\n",
    "    @property\n",
    "    def model_logs_dir(self):\n",
    "        model_logs_dir = os.path.join(self.logs_dir, self.model_name)\n",
    "        if not os.path.exists(model_logs_dir):\n",
    "            os.makedirs(model_logs_dir)\n",
    "        return model_logs_dir\n",
    "\n",
    "    @property\n",
    "    def model_plots_dir(self):\n",
    "        model_plots_dir = os.path.join(self.plots_dir, self.model_name)\n",
    "        if not os.path.exists(model_plots_dir):\n",
    "            os.makedirs(model_plots_dir)\n",
    "        return model_plots_dir\n",
    "\n",
    "    def save(self, step):\n",
    "        model_name = self.model_name + \".model\"\n",
    "        self.saver.save(\n",
    "            self.sess,\n",
    "            os.path.join(self.model_logs_dir, model_name),\n",
    "            global_step=step\n",
    "        )\n",
    "\n",
    "    def load(self):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(self.model_logs_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "\n",
    "    def plot_samples(self, preds, targets, figname, stock_sym=None):\n",
    "        def _flatten(seq):\n",
    "            return [x for y in seq for x in y]\n",
    "\n",
    "        truths = _flatten(targets)[-200:]\n",
    "        preds = _flatten(preds)[-200:]\n",
    "        days = range(len(truths))[-200:]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(days, truths, label='truth')\n",
    "        plt.plot(days, preds, label='pred')\n",
    "        plt.legend(loc='upper left', frameon=False)\n",
    "        plt.xlabel(\"day\")\n",
    "        plt.ylabel(\"normalized price\")\n",
    "        plt.ylim((min(truths), max(truths)))\n",
    "        plt.grid(ls='--')\n",
    "\n",
    "        if stock_sym:\n",
    "            plt.title(stock_sym + \" | Last %d days in test\" % len(truths))\n",
    "\n",
    "        plt.savefig(figname, format='png', bbox_inches='tight', transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
