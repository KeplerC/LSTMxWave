{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "random_wav = \"bass_electronic_018-022-100.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "audio = utils.load_audio(random_wav)\n",
    "sample_length = audio.shape[0]\n",
    "print(sample_length)\n",
    "audio = np.concatenate((audio, audio))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJamGDH9JqqH/D6RlUGqnTbcaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66eebacb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(audio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   6.10351562e-05   2.13623047e-04 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'magenta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-624f97d0a8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmagenta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsynth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNSynthDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'magenta'"
     ]
    }
   ],
   "source": [
    "def mul_or_none(a, b):\n",
    "  \"\"\"Return the element wise multiplicative of the inputs.\n",
    "\n",
    "  If either input is None, we return None.\n",
    "\n",
    "  Args:\n",
    "    a: A tensor input.\n",
    "    b: Another tensor input with the same type as a.\n",
    "\n",
    "  Returns:\n",
    "    None if either input is None. Otherwise returns a * b.\n",
    "  \"\"\"\n",
    "  if a is None or b is None:\n",
    "    return None\n",
    "  return a * b\n",
    "\n",
    "def time_to_batch(x, block_size):\n",
    "  \"\"\"Splits time dimension (i.e. dimension 1) of `x` into batches.\n",
    "\n",
    "  Within each batch element, the `k*block_size` time steps are transposed,\n",
    "  so that the `k` time steps in each output batch element are offset by\n",
    "  `block_size` from each other.\n",
    "\n",
    "  The number of input time steps must be a multiple of `block_size`.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor of shape [nb, k*block_size, n] for some natural number k.\n",
    "    block_size: number of time steps (i.e. size of dimension 1) in the output\n",
    "      tensor.\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape [nb*block_size, k, n]\n",
    "  \"\"\"\n",
    "  shape = x.get_shape().as_list()\n",
    "  y = tf.reshape(x, [\n",
    "      shape[0], shape[1] // block_size, block_size, shape[2]\n",
    "  ])\n",
    "  y = tf.transpose(y, [0, 2, 1, 3])\n",
    "  y = tf.reshape(y, [\n",
    "      shape[0] * block_size, shape[1] // block_size, shape[2]\n",
    "  ])\n",
    "  y.set_shape([\n",
    "      mul_or_none(shape[0], block_size), mul_or_none(shape[1], 1. / block_size),\n",
    "      shape[2]\n",
    "  ])\n",
    "  return y\n",
    "\n",
    "\n",
    "def batch_to_time(x, block_size):\n",
    "  \"\"\"Inverse of `time_to_batch(x, block_size)`.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor of shape [nb*block_size, k, n] for some natural number k.\n",
    "    block_size: number of time steps (i.e. size of dimension 1) in the output\n",
    "      tensor.\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape [nb, k*block_size, n].\n",
    "  \"\"\"\n",
    "  shape = x.get_shape().as_list()\n",
    "  y = tf.reshape(x, [shape[0] // block_size, block_size, shape[1], shape[2]])\n",
    "  y = tf.transpose(y, [0, 2, 1, 3])\n",
    "  y = tf.reshape(y, [shape[0] // block_size, shape[1] * block_size, shape[2]])\n",
    "  y.set_shape([mul_or_none(shape[0], 1. / block_size),\n",
    "               mul_or_none(shape[1], block_size),\n",
    "               shape[2]])\n",
    "  return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv_1:0\", shape=(128000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "learning_rate = 0.00001\n",
    "num_lstm = 512\n",
    "with tf.Graph().as_default():\n",
    "    #session_config = tf.Config\n",
    "    with tf.Session() as sess:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_lstm)\n",
    "        #initial_state = tf.zeros([batch_size, lstm.state_size])\n",
    "        \n",
    "#encode with 8 bit mu-law\n",
    "x = audio\n",
    "x_quantized = utils.mu_law(x)\n",
    "x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n",
    "#x_scaled = tf.expand_dims(x_scaled, 2)\n",
    "num_z = 16\n",
    "#sample_size = inputs.shape[1]\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "#loss_func = tf.nn.seq2seq.sequence_loss(self.outputs, self.targets, softmax_loss_fucntion=tf.nn.softmax.cross_entropy_with_logits, \n",
    "#                                        average_across_timesteps = True, average_cross_batches = True)\n",
    "#opt = tf.train.AdamOptimizer(learning__rate = learning_rate, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-08)\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class TextAutoencoder(object):\n",
    "    \"\"\"\n",
    "    Class that encapsulates the encoder-decoder architecture to\n",
    "    reconstruct pieces of text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lstm_units, num_time_steps, embeddings,\n",
    "                 eos, train=True, train_embeddings=False,\n",
    "                 bidirectional=True):\n",
    "        \"\"\"\n",
    "        Initialize the encoder/decoder and creates Tensor objects\n",
    "\n",
    "        :param lstm_units: number of LSTM units\n",
    "        :param num_time_steps: maximum number of time steps, i.e., token\n",
    "            (when using a trained model, this can be None)\n",
    "        :param embeddings: numpy array with initial embeddings\n",
    "        :param eos: index of the EOS symbol in the embedding matrix\n",
    "        :param train_embeddings: whether to adjust embeddings during training\n",
    "        :param bidirectional: whether to create a bidirectional autoencoder\n",
    "            (if False, a simple linear LSTM is used)\n",
    "        \"\"\"\n",
    "        self.eos = eos\n",
    "        self.bidirectional = bidirectional\n",
    "        #self.vocab_size = embeddings.shape[0]\n",
    "        #self.embedding_size = embeddings.shape[1]        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = 16\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        # the sentence is the object to be memorized\n",
    "        self.sentence = tf.placeholder(tf.int32,\n",
    "                                       [None, num_time_steps],\n",
    "                                       'sentence')\n",
    "        self.sentence_size = tf.placeholder(tf.int32, [None],\n",
    "                                            'sentence_size')\n",
    "        self.l2_constant = tf.placeholder(tf.float32, name='l2_constant')\n",
    "        self.clip_value = tf.placeholder(tf.float32, name='clip')\n",
    "        self.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        self.dropout_keep = tf.placeholder(tf.float32, name='dropout_keep')\n",
    "\n",
    "        self.decoder_step_input = tf.placeholder(tf.int32,\n",
    "                                                 [None],\n",
    "                                                 'prediction_step')\n",
    "\n",
    "        # backwards compatibility with previously saved models\n",
    "        name = 'decoder_fw_step_state_c' if bidirectional \\\n",
    "            else 'decoder_step_state_c'\n",
    "        self.decoder_fw_step_c = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units], name)\n",
    "        name = 'decoder_fw_step_state_h' if bidirectional \\\n",
    "            else 'decoder_step_state_h'\n",
    "        self.decoder_fw_step_h = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units], name)\n",
    "        self.decoder_bw_step_c = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units],\n",
    "                                                'decoder_bw_step_state_c')\n",
    "        self.decoder_bw_step_h = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units],\n",
    "                                                'decoder_bw_step_state_h')\n",
    "\n",
    "        with tf.variable_scope('autoencoder') as self.scope:\n",
    "            self.embeddings = tf.Variable(embeddings, name='embeddings',\n",
    "                                          trainable=train_embeddings)\n",
    "\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            self.lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units,\n",
    "                                                   initializer=initializer)\n",
    "            self.lstm_bw = tf.nn.rnn_cell.LSTMCell(lstm_units,\n",
    "                                                   initializer=initializer)\n",
    "            shape = (2 * lstm_units, self.vocab_size) if bidirectional \\\n",
    "                else (lstm_units, self.vocab_size)\n",
    "            self.projection_w = tf.get_variable('projection_w', shape,\n",
    "                                                initializer=initializer)\n",
    "            initializer = tf.zeros_initializer((self.vocab_size,))\n",
    "            self.projection_b = tf.get_variable('projection_b',\n",
    "                                                initializer=initializer)\n",
    "            embedded = tf.nn.embedding_lookup(self.embeddings, self.sentence)\n",
    "            embedded = tf.nn.dropout(embedded, self.dropout_keep)\n",
    "\n",
    "            if bidirectional:\n",
    "                bdr = tf.nn.bidirectional_dynamic_rnn\n",
    "                ret = bdr(self.lstm_fw, self.lstm_bw,\n",
    "                          embedded, dtype=tf.float32,\n",
    "                          sequence_length=self.sentence_size,\n",
    "                          scope=self.scope)\n",
    "            else:\n",
    "                ret = tf.nn.dynamic_rnn(self.lstm_fw, embedded,\n",
    "                                        dtype=tf.float32,\n",
    "                                        sequence_length=self.sentence_size,\n",
    "                                        scope=self.scope)\n",
    "            _, self.encoded_state = ret\n",
    "            if bidirectional:\n",
    "                encoded_state_fw, encoded_state_bw = self.encoded_state\n",
    "\n",
    "                # set the scope name used inside the decoder.\n",
    "                # maybe there's a more elegant way to do it?\n",
    "                fw_scope_name = self.scope.name + '/FW'\n",
    "                bw_scope_name = self.scope.name + '/BW'\n",
    "            else:\n",
    "                encoded_state_fw = self.encoded_state\n",
    "                fw_scope_name = self.scope\n",
    "\n",
    "            self.scope.reuse_variables()\n",
    "\n",
    "        if train:\n",
    "            # seq2seq functions need lists as input\n",
    "            list_input = self._tensor_to_list(embedded)\n",
    "\n",
    "            # generate a batch of embedded EOS\n",
    "            # sentence_size has the batch dimension\n",
    "            eos_batch = self._generate_batch_eos(self.sentence_size)\n",
    "            embedded_eos = tf.nn.embedding_lookup(self.embeddings,\n",
    "                                                  eos_batch)\n",
    "            decoder_input = [embedded_eos] + list_input\n",
    "\n",
    "            # We give the same inputs to the forward and backward LSTMs,\n",
    "            # but each one has its own hidden state\n",
    "            # their outputs are concatenated and fed to the softmax layer\n",
    "\n",
    "            # The BW LSTM sees the input in reverse order but make predictions\n",
    "            # in forward order\n",
    "            with tf.variable_scope(fw_scope_name, reuse=True) as fw_scope:\n",
    "                res = tf.nn.seq2seq.rnn_decoder(decoder_input,\n",
    "                                                encoded_state_fw,\n",
    "                                                self.lstm_fw,\n",
    "                                                scope=fw_scope)\n",
    "                decoder_outputs_fw, _ = res\n",
    "\n",
    "            if bidirectional:\n",
    "                with tf.variable_scope(bw_scope_name, reuse=True) as bw_scope:\n",
    "                    res = tf.nn.seq2seq.rnn_decoder(decoder_input,\n",
    "                                                    encoded_state_bw,\n",
    "                                                    self.lstm_bw,\n",
    "                                                    scope=bw_scope)\n",
    "                    decoder_outputs_bw, _ = res\n",
    "\n",
    "            # decoder_outputs has the raw outputs before projection\n",
    "            # it has shape (batch, lstm_units)\n",
    "            self.decoder_outputs = []\n",
    "            raw_outputs = zip(decoder_outputs_fw, decoder_outputs_bw) \\\n",
    "                if bidirectional else decoder_outputs_fw\n",
    "            for output in raw_outputs:\n",
    "                if bidirectional:\n",
    "                    # here, each output is (output_fw, output_bw)\n",
    "                    output = tf.concat(1, output)\n",
    "                dropout = tf.nn.dropout(output, self.dropout_keep)\n",
    "                self.decoder_outputs.append(dropout)\n",
    "\n",
    "        # tensors for running a model\n",
    "        embedded_step = tf.nn.embedding_lookup(self.embeddings,\n",
    "                                               self.decoder_step_input)\n",
    "        state_fw = tf.nn.rnn_cell.LSTMStateTuple(self.decoder_fw_step_c,\n",
    "                                                 self.decoder_fw_step_h)\n",
    "        state_bw = tf.nn.rnn_cell.LSTMStateTuple(self.decoder_bw_step_c,\n",
    "                                                 self.decoder_bw_step_h)\n",
    "        with tf.variable_scope(fw_scope_name, reuse=True):\n",
    "            ret_fw = self.lstm_fw(embedded_step, state_fw)\n",
    "        step_output_fw, self.decoder_fw_step_state = ret_fw\n",
    "\n",
    "        if bidirectional:\n",
    "            with tf.variable_scope(bw_scope_name, reuse=True):\n",
    "                ret_bw = self.lstm_bw(embedded_step, state_bw)\n",
    "                step_output_bw, self.decoder_bw_step_state = ret_bw\n",
    "                step_output = tf.concat(1, [step_output_fw, step_output_bw])\n",
    "        else:\n",
    "            step_output = step_output_fw\n",
    "        self.projected_step_output = tf.nn.xw_plus_b(step_output,\n",
    "                                                     self.projection_w,\n",
    "                                                     self.projection_b)\n",
    "\n",
    "        if train:\n",
    "            self._create_training_tensors()\n",
    "\n",
    "    def _tensor_to_list(self, tensor, num_steps=None):\n",
    "        \"\"\"\n",
    "        Splits the input tensor sentence into a list of 1-d\n",
    "        tensors, as much as the number of time steps.\n",
    "        This is necessary for seq2seq functions.\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.num_time_steps\n",
    "        return [tf.squeeze(step, [1])\n",
    "                for step in tf.split(1, num_steps, tensor)]\n",
    "\n",
    "    def _create_training_tensors(self):\n",
    "        \"\"\"\n",
    "        Create member variables related to training.\n",
    "        \"\"\"\n",
    "        sentence_as_list = self._tensor_to_list(self.sentence)\n",
    "        eos_batch = self._generate_batch_eos(sentence_as_list[0])\n",
    "        decoder_labels = sentence_as_list + [eos_batch]\n",
    "        decoder_labels = [tf.cast(step, tf.int64) for step in decoder_labels]\n",
    "\n",
    "        # set the importance of each time step\n",
    "        # 1 if before sentence end or EOS itself; 0 otherwise\n",
    "        label_weights = [tf.cast(tf.less(i - 1, self.sentence_size),\n",
    "                                 tf.float32)\n",
    "                         for i in range(self.num_time_steps + 1)]\n",
    "\n",
    "        projection_w_t = tf.transpose(self.projection_w)\n",
    "\n",
    "        def loss_function(inputs, labels):\n",
    "            labels = tf.reshape(labels, (-1, 1))\n",
    "            return tf.nn.sampled_softmax_loss(projection_w_t,\n",
    "                                              self.projection_b,\n",
    "                                              inputs, labels,\n",
    "                                              100, self.vocab_size)\n",
    "        labeled_loss = tf.nn.seq2seq.sequence_loss(self.decoder_outputs,\n",
    "                                                   decoder_labels,\n",
    "                                                   label_weights,\n",
    "                                                   softmax_loss_function=loss_function)\n",
    "        # self.loss = labeled_loss + self.compute_l2_loss()\n",
    "        self.loss = labeled_loss\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(self.loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, self.clip_value)\n",
    "\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, v),\n",
    "                                                  global_step=self.global_step)\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        \"\"\"\n",
    "        Return all trainable variables inside the model\n",
    "        \"\"\"\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                 self.scope.name)\n",
    "\n",
    "    def train(self, session, save_path, train_data, valid_data,\n",
    "              batch_size, epochs, learning_rate, dropout_keep,\n",
    "              clip_value, report_interval):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        :param session: tensorflow session\n",
    "        :param train_data: Dataset object with training data\n",
    "        :param valid_data: Dataset object with validation data\n",
    "        :param batch_size: batch size\n",
    "        :param learning_rate: initial learning rate\n",
    "        :param dropout_keep: the probability that each LSTM input/output is kept\n",
    "        :param epochs: how many epochs to train for\n",
    "        :param clip_value: value to clip tensor norm during training\n",
    "        :param save_path: folder to save the model\n",
    "        :param report_interval: report after that many batches\n",
    "        \"\"\"\n",
    "        saver = tf.train.Saver(self.get_trainable_variables(),\n",
    "                               max_to_keep=1)\n",
    "\n",
    "        best_loss = 10000\n",
    "        accumulated_loss = 0\n",
    "        batch_counter = 0\n",
    "\n",
    "        # get all data at once. we need all matrices with the same size,\n",
    "        # or else they don't fit the placeholders\n",
    "        train_sents, train_sizes = train_data.join_all(self.eos,\n",
    "                                                       self.num_time_steps,\n",
    "                                                       shuffle=True)\n",
    "\n",
    "        del train_data  # save memory...\n",
    "        valid_sents, valid_sizes = valid_data.join_all(self.eos,\n",
    "                                                       self.num_time_steps,\n",
    "                                                       shuffle=True)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # idx1 and idx2 are used to index where each batch begins and\n",
    "            # ends in train_data\n",
    "            idx1 = 0\n",
    "            while idx1 < len(train_sents):\n",
    "                idx2 = idx1 + batch_size\n",
    "                feeds = {self.sentence: train_sents[idx1:idx2],\n",
    "                         self.sentence_size: train_sizes[idx1:idx2],\n",
    "                         self.clip_value: clip_value,\n",
    "                         self.dropout_keep: dropout_keep,\n",
    "                         self.learning_rate: learning_rate}\n",
    "\n",
    "                _, loss = session.run([self.train_op, self.loss], feeds)\n",
    "                # tl = timeline.Timeline(run_metadata.step_stats)\n",
    "                # ctf = tl.generate_chrome_trace_format()\n",
    "                # with open('timeline-%d.json' % batch_counter, 'wb') as f:\n",
    "                #     f.write(ctf)\n",
    "\n",
    "                accumulated_loss += loss\n",
    "\n",
    "                idx1 = idx2\n",
    "                batch_counter += 1\n",
    "                if batch_counter % report_interval == 0:\n",
    "                    avg_loss = accumulated_loss / report_interval\n",
    "                    accumulated_loss = 0\n",
    "\n",
    "                    # we can't use all the validation at once, since it would\n",
    "                    # take too much memory. running many small batches would\n",
    "                    # instead take too much time. So let's just sample it.\n",
    "                    sample_indices = np.random.randint(0, len(valid_data),\n",
    "                                                       5000)\n",
    "                    feeds = {self.sentence: valid_sents[sample_indices],\n",
    "                             self.sentence_size: valid_sizes[sample_indices],\n",
    "                             self.dropout_keep: 1}\n",
    "\n",
    "                    loss = session.run(self.loss, feeds)\n",
    "                    msg = '%d epochs, %d batches\\t' % (i, batch_counter)\n",
    "                    msg += 'Avg batch loss: %f\\t' % avg_loss\n",
    "                    msg += 'Validation loss: %f' % loss\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        self.save(saver, session, save_path)\n",
    "                        msg += '\\t(saved model)'\n",
    "\n",
    "                    logging.info(msg)\n",
    "\n",
    "    def save(self, saver, session, directory):\n",
    "        \"\"\"\n",
    "        Save the autoencoder model and metadata to the specified\n",
    "        directory.\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(directory, 'model')\n",
    "        saver.save(session, model_path)\n",
    "        metadata = {'num_time_steps': self.num_time_steps,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'embedding_size': self.embedding_size,\n",
    "                    'num_units': self.lstm_fw.output_size,\n",
    "                    'eos': self.eos,\n",
    "                    'bidirectional': self.bidirectional\n",
    "                    }\n",
    "        metadata_path = os.path.join(directory, 'metadata.json')\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            json.dump(metadata, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, directory, session, train=False):\n",
    "        \"\"\"\n",
    "        Load an instance of this class from a previously saved one.\n",
    "        :param directory: directory with the model files\n",
    "        :param session: tensorflow session\n",
    "        :param train: if True, also create training tensors\n",
    "        :return: a TextAutoencoder instance\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(directory, 'model')\n",
    "        metadata_path = os.path.join(directory, 'metadata.json')\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadata = json.load(f)\n",
    "        num_time_steps = metadata['num_time_steps'] if train else None\n",
    "        dummy_embeddings = np.empty((metadata['vocab_size'],\n",
    "                                     metadata['embedding_size'],),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "        ae = TextAutoencoder(metadata['num_units'], num_time_steps,\n",
    "                             dummy_embeddings,\n",
    "                             metadata['eos'], train=train,\n",
    "                             bidirectional=metadata['bidirectional'])\n",
    "        vars_to_load = ae.get_trainable_variables()\n",
    "        if not train:\n",
    "            # if not flagged for training, the embeddings won't be in\n",
    "            # the list\n",
    "            vars_to_load.append(ae.embeddings)\n",
    "\n",
    "        saver = tf.train.Saver(vars_to_load)\n",
    "        saver.restore(session, model_path)\n",
    "        return ae\n",
    "\n",
    "    def encode(self, session, inputs, sizes):\n",
    "        \"\"\"\n",
    "        Run the encoder to obtain the encoded hidden state\n",
    "\n",
    "        :param session: tensorflow session\n",
    "        :param inputs: 2-d array with the word indices\n",
    "        :param sizes: 1-d array with size of each sentence\n",
    "        :return: a 2-d numpy array with the hidden state\n",
    "        \"\"\"\n",
    "        feeds = {self.sentence: inputs,\n",
    "                 self.sentence_size: sizes,\n",
    "                 self.dropout_keep: 1}\n",
    "        state = session.run(self.encoded_state, feeds)\n",
    "        if self.bidirectional:\n",
    "            state_fw, state_bw = state\n",
    "            return np.hstack((state_fw.c, state_bw.c))\n",
    "        return state.c\n",
    "\n",
    "    def run(self, session, inputs, sizes):\n",
    "        \"\"\"\n",
    "        Run the autoencoder with the given data\n",
    "\n",
    "        :param session: tensorflow session\n",
    "        :param inputs: 2-d array with the word indices\n",
    "        :param sizes: 1-d array with size of each sentence\n",
    "        :return: a 2-d array (batch, output_length) with the answer\n",
    "            produced by the autoencoder. The output length is not\n",
    "            fixed; it stops after producing EOS for all items in the\n",
    "            batch or reaching two times the maximum number of time\n",
    "            steps in the inputs.\n",
    "        \"\"\"\n",
    "        feeds = {self.sentence: inputs,\n",
    "                 self.sentence_size: sizes,\n",
    "                 self.dropout_keep: 1}\n",
    "        state = session.run(self.encoded_state, feeds)\n",
    "        if self.bidirectional:\n",
    "            state_fw, state_bw = state\n",
    "        else:\n",
    "            state_fw = state\n",
    "\n",
    "        time_steps = 0\n",
    "        max_time_steps = 2 * len(inputs[0])\n",
    "        answer = []\n",
    "        input_symbol = self.eos * np.ones_like(sizes, dtype=np.int32)\n",
    "\n",
    "        # this array control which sequences have already been finished by the\n",
    "        # decoder, i.e., for which ones it already produced the END symbol\n",
    "        sequences_done = np.zeros_like(sizes, dtype=np.bool)\n",
    "\n",
    "        while True:\n",
    "            # we could use tensorflow's rnn_decoder, but this gives us\n",
    "            # finer control\n",
    "\n",
    "            feeds = {self.decoder_fw_step_c: state_fw.c,\n",
    "                     self.decoder_fw_step_h: state_fw.h,\n",
    "                     self.decoder_step_input: input_symbol,\n",
    "                     self.dropout_keep: 1}\n",
    "            if self.bidirectional:\n",
    "                feeds[self.decoder_bw_step_c] = state_bw.c\n",
    "                feeds[self.decoder_bw_step_h] = state_bw.h\n",
    "\n",
    "                ops = [self.projected_step_output,\n",
    "                       self.decoder_fw_step_state,\n",
    "                       self.decoder_bw_step_state]\n",
    "                outputs, state_fw, state_bw = session.run(ops, feeds)\n",
    "            else:\n",
    "                ops = [self.projected_step_output,\n",
    "                       self.decoder_fw_step_state]\n",
    "                outputs, state_fw = session.run(ops, feeds)\n",
    "\n",
    "            input_symbol = outputs.argmax(1)\n",
    "            answer.append(input_symbol)\n",
    "\n",
    "            # use an \"additive\" or in order to avoid infinite loops\n",
    "            sequences_done |= (input_symbol == self.eos)\n",
    "\n",
    "            if sequences_done.all() or time_steps > max_time_steps:\n",
    "                break\n",
    "            else:\n",
    "                time_steps += 1\n",
    "\n",
    "        return np.hstack(answer)\n",
    "\n",
    "    def _generate_batch_eos(self):\n",
    "        \"\"\"\n",
    "        Generate a 1-d tensor with copies of EOS as big as the batch size,\n",
    "\n",
    "        :param like: a tensor whose shape the returned embeddings should match\n",
    "        :return: a tensor with shape as `like`\n",
    "        \"\"\"\n",
    "        return get_batch(512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-26f715de9fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                         \u001b[0mx_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                         bidirectional=False)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f2c8214d290f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lstm_units, num_time_steps, embeddings, eos, train, train_embeddings, bidirectional)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoencoder'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             self.embeddings = tf.Variable(embeddings, name='embeddings',\n\u001b[0;32m---> 73\u001b[0;31m                                           trainable=train_embeddings)\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `constraint` argument must be a callable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINABLE_VARIABLES\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m       \u001b[0mcollections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINABLE_VARIABLES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "model = TextAutoencoder(512, 32,\n",
    "                                        x_scaled,None,\n",
    "                                        train_embeddings=audio,\n",
    "                                        bidirectional=False)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
