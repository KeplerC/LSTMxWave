{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "random_wav = \"bass_electronic_018-022-100.wav\"\n",
    "\n",
    "\n",
    "audio = utils.load_audio(random_wav)\n",
    "sample_length = audio.shape[0]\n",
    "print(sample_length)\n",
    "audio = np.concatenate((audio, audio))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJamGDH9JqqH/D6RlUGqnTbcaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12722f01d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(audio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   6.10351562e-05   2.13623047e-04 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mul_or_none(a, b):\n",
    "  \"\"\"Return the element wise multiplicative of the inputs.\n",
    "\n",
    "  If either input is None, we return None.\n",
    "\n",
    "  Args:\n",
    "    a: A tensor input.\n",
    "    b: Another tensor input with the same type as a.\n",
    "\n",
    "  Returns:\n",
    "    None if either input is None. Otherwise returns a * b.\n",
    "  \"\"\"\n",
    "  if a is None or b is None:\n",
    "    return None\n",
    "  return a * b\n",
    "\n",
    "def time_to_batch(x, block_size):\n",
    "  \"\"\"Splits time dimension (i.e. dimension 1) of `x` into batches.\n",
    "\n",
    "  Within each batch element, the `k*block_size` time steps are transposed,\n",
    "  so that the `k` time steps in each output batch element are offset by\n",
    "  `block_size` from each other.\n",
    "\n",
    "  The number of input time steps must be a multiple of `block_size`.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor of shape [nb, k*block_size, n] for some natural number k.\n",
    "    block_size: number of time steps (i.e. size of dimension 1) in the output\n",
    "      tensor.\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape [nb*block_size, k, n]\n",
    "  \"\"\"\n",
    "  shape = x.get_shape().as_list()\n",
    "  y = tf.reshape(x, [\n",
    "      shape[0], shape[1] // block_size, block_size, shape[2]\n",
    "  ])\n",
    "  y = tf.transpose(y, [0, 2, 1, 3])\n",
    "  y = tf.reshape(y, [\n",
    "      shape[0] * block_size, shape[1] // block_size, shape[2]\n",
    "  ])\n",
    "  y.set_shape([\n",
    "      mul_or_none(shape[0], block_size), mul_or_none(shape[1], 1. / block_size),\n",
    "      shape[2]\n",
    "  ])\n",
    "  return y\n",
    "\n",
    "\n",
    "def batch_to_time(x, block_size):\n",
    "  \"\"\"Inverse of `time_to_batch(x, block_size)`.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor of shape [nb*block_size, k, n] for some natural number k.\n",
    "    block_size: number of time steps (i.e. size of dimension 1) in the output\n",
    "      tensor.\n",
    "\n",
    "  Returns:\n",
    "    Tensor of shape [nb, k*block_size, n].\n",
    "  \"\"\"\n",
    "  shape = x.get_shape().as_list()\n",
    "  y = tf.reshape(x, [shape[0] // block_size, block_size, shape[1], shape[2]])\n",
    "  y = tf.transpose(y, [0, 2, 1, 3])\n",
    "  y = tf.reshape(y, [shape[0] // block_size, shape[1] * block_size, shape[2]])\n",
    "  y.set_shape([mul_or_none(shape[0], 1. / block_size),\n",
    "               mul_or_none(shape[1], block_size),\n",
    "               shape[2]])\n",
    "  return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-afedf79e884b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#encode with 8 bit mu-law\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mx_quantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_law\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_quantized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m128.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "learning_rate = 0.00001\n",
    "num_lstm = 512\n",
    "with tf.Graph().as_default():\n",
    "    #session_config = tf.Config\n",
    "    with tf.Session() as sess:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_lstm)\n",
    "        #initial_state = tf.zeros([batch_size, lstm.state_size])\n",
    "        \n",
    "#encode with 8 bit mu-law\n",
    "x = audio\n",
    "x_quantized = utils.mu_law(x)\n",
    "x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n",
    "#x_scaled = tf.expand_dims(x_scaled, 2)\n",
    "num_z = 16\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "num_steps = 1\n",
    "input_size = 1\n",
    "lstm_size = 128\n",
    "keep_prob = 1\n",
    "stock_count = 1\n",
    "embed_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keplerc/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#official tutorial: words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_features])\n",
    "learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "# symbols are mapped to integers.\n",
    "symbols = tf.placeholder(tf.int32, [None, 1], name='symbols')\n",
    "inputs = tf.placeholder(tf.float32, [None, num_steps, input_size], name=\"inputs\")\n",
    "\n",
    "#inputs = tf.reshape(inputs, tf.TensorShape([64000,1,1]))\n",
    "targets = tf.placeholder(tf.float32, [None, input_size], name=\"targets\")\n",
    "\n",
    "def _create_one_cell():\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "    '''\n",
    "    if keep_prob < 1.0:\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)    '''\n",
    "    return lstm_cell\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([_create_one_cell() for _ in range(num_layers)],state_is_tuple=True\n",
    ") if num_layers > 1 else _create_one_cell()\n",
    "\n",
    "\n",
    "#embedding layer\n",
    "embed_matrix = tf.Variable(\n",
    "    tf.random_uniform([stock_count, embed_size], -1.0, 1.0),\n",
    "    name=\"embed_matrix\"\n",
    ")\n",
    "sym_embeds = tf.nn.embedding_lookup(embed_matrix, symbols)\n",
    "            \n",
    "# stock_label_embeds.shape = (batch_size, embedding_size)\n",
    "stacked_symbols = tf.tile(symbols, [1, num_steps], name='stacked_stock_labels')\n",
    "stacked_embeds = tf.nn.embedding_lookup(embed_matrix, stacked_symbols)\n",
    "\n",
    "# After concat, inputs.shape = (batch_size, num_steps, lstm_size + embed_size)\n",
    "inputs_with_embed = tf.concat([inputs, stacked_embeds], axis=2, name=\"inputs_with_embed\")\n",
    "\n",
    "val, state_ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, scope=\"dynamic_rnn\")\n",
    "\n",
    "# Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size)\n",
    "# After transpose, val.get_shape() = (num_steps, batch_size, lstm_size)\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1, name=\"lstm_state\")\n",
    "ws = tf.Variable(tf.truncated_normal([lstm_size, input_size]), name=\"w\")\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[input_size]), name=\"b\")\n",
    "pred = tf.matmul(last, ws) + bias\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(pred - targets), name=\"loss_mse\")\n",
    "optim = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, name=\"rmsprop_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio = utils.load_audio(random_wav)\n",
    "audio = list(map(lambda x: x, audio))\n",
    "np.array(audio)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#normalized does not work\n",
    "class dt():\n",
    "    def __init__(self, raw_audio, input_size = input_size, num_steps = num_steps, test_ratio = 0.1, normalized = False):\n",
    "        self.raw_seq = np.array(raw_audio)\n",
    "        self.input_size = input_size\n",
    "        self.num_steps = num_steps \n",
    "        self.test_ratio = test_ratio\n",
    "        self.normalized = normalized\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self._prepare_data(self.raw_seq)\n",
    "\n",
    "    def _prepare_data(self, seq):\n",
    "        # split into items of input_size\n",
    "        seq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size])\n",
    "               for i in range(len(seq) // self.input_size)]\n",
    "        \n",
    "        if self.normalized:\n",
    "            seq = [seq[0] / seq[0][0] - 1.0] + [\n",
    "                curr / seq[i][-1] - 1.0 for i, curr in enumerate(seq[1:])]\n",
    "\n",
    "        # split into groups of num_steps\n",
    "        X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "        y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.test_ratio))\n",
    "        train_X, test_X = X[:train_size], X[train_size:]\n",
    "        train_y, test_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, test_X, test_y\n",
    "\n",
    "    def generate_one_epoch(self, batch_size):\n",
    "        num_batches = int(len(self.train_X)) // batch_size\n",
    "        if batch_size * num_batches < len(self.train_X):\n",
    "            num_batches += 1\n",
    "\n",
    "        batch_indices = list(range(num_batches))\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = self.train_X[j * batch_size: (j + 1) * batch_size]\n",
    "            batch_y = self.train_y[j * batch_size: (j + 1) * batch_size]\n",
    "            assert set(map(len, batch_X)) == {self.num_steps}\n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels = np.ones([100,1])\n",
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00999953\n",
      "0.00999666\n",
      "0.00999219\n",
      "0.00998724\n",
      "0.009985\n",
      "0.00998053\n",
      "0.00997603\n",
      "0.00999644\n",
      "0.00996763\n",
      "0.00996277\n",
      "0.0099574\n",
      "0.00893784\n",
      "0.00994626\n",
      "0.0211907\n",
      "0.00994362\n",
      "0.00993945\n",
      "0.00992828\n",
      "0.00992072\n",
      "0.20389\n",
      "0.00988702\n",
      "0.00987776\n",
      "0.00986855\n",
      "0.00985783\n",
      "0.00984943\n",
      "0.00983936\n",
      "0.00982879\n",
      "0.00981717\n",
      "0.00980514\n",
      "0.0174457\n",
      "0.0097778\n",
      "0.00976214\n",
      "0.00975127\n",
      "0.00973348\n",
      "0.00971814\n",
      "0.00970173\n",
      "0.00972249\n",
      "0.00966871\n",
      "0.00965163\n",
      "0.0150719\n",
      "0.00960802\n",
      "0.0095884\n",
      "0.00956742\n",
      "0.00954473\n",
      "0.00952247\n",
      "0.00949831\n",
      "0.009474\n",
      "0.00944837\n",
      "0.00942309\n",
      "0.00939853\n",
      "0.00936782\n",
      "0.00934076\n",
      "0.00931082\n",
      "0.00927909\n",
      "0.00925154\n",
      "0.00922064\n",
      "0.00919056\n",
      "0.00915393\n",
      "0.00912217\n",
      "0.00908788\n",
      "0.00905298\n",
      "0.00901794\n",
      "0.0089807\n",
      "0.00894455\n",
      "0.00890856\n",
      "0.00887191\n",
      "0.00883323\n",
      "0.00879512\n",
      "0.00875661\n",
      "0.0087181\n",
      "0.00867806\n",
      "0.00863848\n",
      "0.00860111\n",
      "0.0085596\n",
      "0.00851877\n",
      "0.00842772\n",
      "0.00843762\n",
      "0.00839411\n",
      "0.00835392\n",
      "0.00831275\n",
      "0.0082715\n",
      "0.0082285\n",
      "0.0081876\n",
      "0.00814522\n",
      "0.00810377\n",
      "0.00806061\n",
      "0.00793197\n",
      "0.0079766\n",
      "0.00793411\n",
      "0.00788791\n",
      "0.00784889\n",
      "0.00780701\n",
      "0.00776405\n",
      "0.00772289\n",
      "0.00767998\n",
      "0.00763815\n",
      "0.00759641\n",
      "0.00736948\n",
      "0.00751294\n",
      "0.00747135\n",
      "0.00742838\n",
      "0.00738662\n",
      "0.00734458\n",
      "0.00730217\n",
      "0.00726151\n",
      "0.00721993\n",
      "0.00717889\n",
      "0.00713754\n",
      "0.00709523\n",
      "0.00708184\n",
      "0.00701315\n",
      "0.006973\n",
      "0.00693155\n",
      "0.00689056\n",
      "0.00684963\n",
      "0.00680847\n",
      "0.0067682\n",
      "0.00672931\n",
      "0.00668876\n",
      "0.00664804\n",
      "0.00660729\n",
      "0.00656743\n",
      "0.00652855\n",
      "0.00648906\n",
      "0.00644902\n",
      "0.00640891\n",
      "0.00637068\n",
      "0.00633071\n",
      "0.00629218\n",
      "0.00625218\n",
      "0.00621281\n",
      "0.00617567\n",
      "0.00613776\n",
      "0.0060987\n",
      "0.00605936\n",
      "0.00602134\n",
      "0.00598345\n",
      "0.00594352\n",
      "0.00590688\n",
      "0.00586971\n",
      "0.0058328\n",
      "0.00579391\n",
      "0.0057572\n",
      "0.0057188\n",
      "0.00568296\n",
      "0.00564749\n",
      "0.00561027\n",
      "0.00557228\n",
      "0.00553465\n",
      "0.0054994\n",
      "0.00546175\n",
      "0.00542652\n",
      "0.00539\n",
      "0.00535479\n",
      "0.00531873\n",
      "0.0052825\n",
      "0.00524691\n",
      "0.00521166\n",
      "0.00517525\n",
      "0.0051423\n",
      "0.0051057\n",
      "0.00507039\n",
      "0.00503558\n",
      "0.00500031\n",
      "0.00496558\n",
      "0.00493127\n",
      "0.00486584\n",
      "0.00486311\n",
      "0.00482762\n",
      "0.00479399\n",
      "0.0181705\n",
      "0.00458488\n",
      "0.00467626\n",
      "0.00464683\n",
      "0.00461425\n",
      "0.00458213\n",
      "0.00455085\n",
      "0.00451827\n",
      "0.00448831\n",
      "0.00445589\n",
      "0.00442458\n",
      "0.00439222\n",
      "0.00436219\n",
      "0.00433143\n",
      "0.00430024\n",
      "0.00426676\n",
      "0.00423656\n",
      "0.00420459\n",
      "0.00417489\n",
      "0.00414236\n",
      "0.00411237\n",
      "0.00355774\n",
      "0.00405031\n",
      "0.00401921\n",
      "0.00398811\n",
      "0.00395458\n",
      "0.00392729\n",
      "0.00389719\n",
      "0.00386619\n",
      "0.00383663\n",
      "0.00380649\n",
      "0.00377609\n",
      "0.00374673\n",
      "0.00371613\n",
      "0.00368572\n",
      "0.00365639\n",
      "0.00362655\n",
      "0.00359694\n",
      "0.00356526\n",
      "0.00354029\n",
      "0.0035099\n",
      "0.00348063\n",
      "0.00345201\n",
      "0.00342302\n",
      "0.00339419\n",
      "0.00336579\n",
      "0.00333755\n",
      "0.00330942\n",
      "0.00328132\n",
      "0.00325378\n",
      "0.00322618\n",
      "0.00319904\n",
      "0.0031697\n",
      "0.00343351\n",
      "0.00311545\n",
      "0.00308852\n",
      "0.00306171\n",
      "0.00303457\n",
      "0.00300739\n",
      "0.00298125\n",
      "0.00295342\n",
      "0.00232661\n",
      "0.00290391\n",
      "0.00287717\n",
      "0.00285006\n",
      "0.00282417\n",
      "0.00279857\n",
      "0.0027738\n",
      "0.00274617\n",
      "0.00272081\n",
      "0.00269527\n",
      "0.00267042\n",
      "0.00264469\n",
      "0.00261962\n",
      "0.00259522\n",
      "0.00610929\n",
      "0.00208385\n",
      "0.00255873\n",
      "0.00253359\n",
      "0.00251885\n",
      "0.00248245\n",
      "0.00245768\n",
      "0.00243316\n",
      "0.00240835\n",
      "0.00238348\n",
      "0.00235946\n",
      "0.00233562\n",
      "0.00231104\n",
      "0.00228689\n",
      "0.0022639\n",
      "0.00224145\n",
      "0.00221828\n",
      "0.00219376\n",
      "0.00217118\n",
      "0.00214876\n",
      "0.0021261\n",
      "0.00210293\n",
      "0.00208099\n",
      "0.00205837\n",
      "0.00203609\n",
      "0.00201455\n",
      "0.00236547\n",
      "0.00197008\n",
      "0.00194838\n",
      "0.00192741\n",
      "0.00190656\n",
      "0.00188482\n",
      "0.00186352\n",
      "0.00184291\n",
      "0.00182288\n",
      "0.00180188\n",
      "0.00178125\n",
      "0.00176112\n",
      "0.00174023\n",
      "0.00172037\n",
      "0.00170024\n",
      "0.00168053\n",
      "0.00127155\n",
      "0.00164341\n",
      "0.00162363\n",
      "0.00160399\n",
      "0.0017961\n",
      "0.0014622\n",
      "0.00154623\n",
      "0.00152782\n",
      "0.00150813\n",
      "0.0014896\n",
      "0.00147143\n",
      "0.00145186\n",
      "0.00143398\n",
      "0.00141548\n",
      "0.0013974\n",
      "0.00137925\n",
      "0.00136187\n",
      "0.00134384\n",
      "0.00104047\n",
      "0.00131004\n",
      "0.00129303\n",
      "0.000852194\n",
      "0.00126138\n",
      "0.00124456\n",
      "0.00122662\n",
      "0.00120967\n",
      "0.00119253\n",
      "0.00117558\n",
      "0.00115899\n",
      "0.00114343\n",
      "0.00112662\n",
      "0.00111032\n",
      "0.00109387\n",
      "0.0010783\n",
      "0.00106252\n",
      "0.00104712\n",
      "0.00103117\n",
      "0.0010162\n",
      "0.00100061\n",
      "0.000985565\n",
      "0.000970536\n",
      "0.000955122\n",
      "0.000941467\n",
      "0.000926324\n",
      "0.000911913\n",
      "0.000897285\n",
      "0.000883301\n",
      "0.00086912\n",
      "0.00085515\n",
      "0.000841287\n",
      "0.000827342\n",
      "0.000813508\n",
      "0.000801178\n",
      "0.000787192\n",
      "0.000773938\n",
      "0.000760544\n",
      "0.000747521\n",
      "0.000735086\n",
      "0.000722489\n",
      "0.00069961\n",
      "0.000696707\n",
      "0.000684498\n",
      "0.000671821\n",
      "0.000660051\n",
      "0.000648271\n",
      "0.000636033\n",
      "0.000624235\n",
      "0.00061266\n",
      "0.00060158\n",
      "0.000589466\n",
      "0.000577947\n",
      "0.000567712\n",
      "0.000555606\n",
      "0.00054469\n",
      "0.000533861\n",
      "0.000523272\n",
      "0.000512636\n",
      "0.000502584\n",
      "0.000491728\n",
      "0.000481603\n",
      "0.000471268\n",
      "0.000461275\n",
      "0.000451825\n",
      "0.000441543\n",
      "0.000431837\n",
      "0.000421932\n",
      "0.000226001\n",
      "0.000405817\n",
      "0.000396603\n",
      "0.000387075\n",
      "0.000378108\n",
      "0.000369163\n",
      "0.000360596\n",
      "0.000351366\n",
      "0.000342579\n",
      "0.000334032\n",
      "0.000325676\n",
      "0.000317422\n",
      "0.000309374\n",
      "0.000301466\n",
      "0.000293521\n",
      "0.000285698\n",
      "0.000278185\n",
      "0.000270432\n",
      "0.000263164\n",
      "0.000255958\n",
      "0.00024866\n",
      "0.00024126\n",
      "0.000234672\n",
      "0.000227861\n",
      "0.0075765\n",
      "0.000193784\n",
      "0.000204666\n",
      "0.0001986\n",
      "0.000192653\n",
      "0.00018706\n",
      "0.00018141\n",
      "0.000176066\n",
      "0.000171231\n",
      "0.000163078\n",
      "0.000159626\n",
      "0.000154206\n",
      "0.000108846\n",
      "0.000144697\n",
      "0.000139667\n",
      "0.000134599\n",
      "0.000129788\n",
      "0.000124863\n",
      "0.000120304\n",
      "0.000115676\n",
      "0.000111198\n",
      "2.99603e-05\n",
      "0.000104705\n",
      "0.000100235\n",
      "9.58586e-05\n",
      "9.17521e-05\n",
      "8.77063e-05\n",
      "8.36913e-05\n",
      "8.00522e-05\n",
      "7.63131e-05\n",
      "7.26074e-05\n",
      "6.93313e-05\n",
      "6.57395e-05\n",
      "6.24538e-05\n",
      "5.95862e-05\n",
      "5.63631e-05\n",
      "5.33118e-05\n",
      "5.03434e-05\n",
      "4.75373e-05\n",
      "4.48016e-05\n",
      "4.22927e-05\n",
      "3.97962e-05\n",
      "3.74146e-05\n",
      "3.50984e-05\n",
      "3.27954e-05\n",
      "3.07381e-05\n",
      "2.86789e-05\n",
      "2.66511e-05\n",
      "2.48793e-05\n",
      "2.30652e-05\n",
      "2.12998e-05\n",
      "1.97498e-05\n",
      "1.82e-05\n",
      "1.66989e-05\n",
      "1.53457e-05\n",
      "1.40656e-05\n",
      "1.27405e-05\n",
      "1.16442e-05\n",
      "1.06813e-05\n",
      "9.59132e-06\n",
      "8.58267e-06\n",
      "7.70808e-06\n",
      "6.91426e-06\n",
      "6.16036e-06\n",
      "5.51056e-06\n",
      "4.78866e-06\n",
      "1.30983e-05\n",
      "5.22126e-06\n",
      "4.51322e-06\n",
      "3.87601e-06\n",
      "3.39011e-06\n",
      "2.88264e-06\n",
      "2.44108e-06\n",
      "2.07145e-06\n",
      "1.73815e-06\n",
      "1.44866e-06\n",
      "1.1945e-06\n",
      "1.01398e-06\n",
      "7.94568e-07\n",
      "6.47062e-07\n",
      "5.08186e-07\n",
      "3.91604e-07\n",
      "3.06882e-07\n",
      "2.3317e-07\n",
      "1.75385e-07\n",
      "1.31765e-07\n",
      "9.37954e-08\n",
      "6.60499e-08\n",
      "0.0079974\n",
      "4.20647e-07\n",
      "1.65222e-07\n",
      "1.4861e-07\n",
      "1.10456e-07\n",
      "9.67624e-08\n",
      "7.25714e-08\n",
      "5.18049e-08\n",
      "4.04448e-08\n",
      "3.21023e-08\n",
      "2.4364e-08\n",
      "1.75651e-08\n",
      "1.33451e-08\n",
      "8.55563e-09\n",
      "5.45414e-09\n",
      "4.62925e-09\n",
      "2.96102e-09\n",
      "1.10016e-09\n",
      "1.57187e-09\n",
      "7.41182e-10\n",
      "9.72711e-10\n",
      "8.54789e-10\n",
      "4.29588e-10\n",
      "4.5679e-11\n",
      "1.1972e-10\n",
      "2.74966e-10\n",
      "1.76181e-10\n",
      "2.58121e-10\n",
      "7.26387e-11\n",
      "2.80313e-11\n",
      "4.71974e-11\n",
      "6.69807e-10\n",
      "2.16417e-10\n",
      "2.68995e-10\n",
      "3.04348e-10\n",
      "3.36288e-10\n",
      "2.90779e-10\n",
      "2.19661e-10\n",
      "2.14247e-10\n",
      "2.23149e-10\n",
      "1.35508e-10\n",
      "2.46984e-10\n",
      "3.23777e-10\n",
      "1.66175e-10\n",
      "1.5544e-11\n",
      "3.00369e-10\n",
      "5.76165e-10\n",
      "3.9524e-10\n",
      "1.67202e-10\n",
      "2.93579e-10\n",
      "6.12595e-06\n",
      "5.81328e-07\n",
      "2.5959e-07\n",
      "1.17319e-07\n",
      "4.99452e-08\n",
      "1.90731e-08\n",
      "6.93694e-09\n",
      "3.05044e-09\n",
      "1.59133e-09\n",
      "3.72891e-10\n",
      "3.41463e-10\n",
      "4.11283e-10\n",
      "2.20033e-10\n",
      "3.0023e-10\n",
      "2.63909e-10\n",
      "2.67807e-10\n",
      "8.42956e-11\n",
      "3.21114e-10\n",
      "1.03029e-10\n",
      "2.12103e-10\n",
      "1.78273e-10\n",
      "3.97536e-10\n",
      "4.86294e-10\n",
      "3.66309e-10\n",
      "9.55107e-06\n",
      "0.00139408\n",
      "1.92248e-06\n",
      "1.27599e-06\n",
      "8.52547e-07\n",
      "5.607e-07\n",
      "3.75041e-07\n",
      "2.44731e-07\n",
      "1.36006e-07\n",
      "8.70099e-08\n",
      "4.95087e-08\n",
      "2.64357e-08\n",
      "1.46419e-08\n",
      "2.41575e-07\n",
      "0.0481285\n",
      "6.62847e-07\n",
      "6.23036e-07\n",
      "5.99408e-07\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    merged_test_X = np.array(audio)\n",
    "    merged_test_y = np.array(audio)\n",
    "\n",
    "    global_step = 0\n",
    "    epoch_step = 0\n",
    "    d = dt(audio)\n",
    "    for batch_x, batch_y in d.generate_one_epoch(batch_size):\n",
    "        global_step += 1\n",
    "        epoch_step += 1\n",
    "        train_data_feed = {\n",
    "            learning_rate : 0.00001,\n",
    "            inputs: batch_x,\n",
    "            targets: batch_y,\n",
    "            symbols: batch_labels,\n",
    "        }\n",
    "        train_loss, _ = sess.run([loss, optim], train_data_feed)\n",
    "        print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
